tag: DATA-PIPELINE
section: instructions
blocks:
  - id: etl-patterns
    tier: recommended
    title: "ETL & Pipeline Design"
    content: |
      ## ETL & Pipeline Orchestration

      - Design every pipeline stage to be idempotent. Re-running a stage with the same input must produce the same output without side effects or duplicates.
      - Use a DAG-based orchestrator (e.g., Airflow, Dagster, Prefect) to define task dependencies, schedule runs, and handle retries with exponential backoff.
      - Partition data by time (daily/hourly) or logical key so that re-processing a failed partition does not require reprocessing the entire dataset.
      - Implement the Extract-Load-Transform (ELT) pattern when the target warehouse supports compute; push transformations into SQL/dbt models for transparency and version control.
      - Tag every pipeline run with a unique run ID. Propagate this ID through all stages and into output metadata for end-to-end lineage tracking.
      - Keep extraction, transformation, and loading logic in separate, independently testable modules. Avoid monolithic scripts that mix concerns.
      - Define SLAs for each pipeline (e.g., "daily sales pipeline completes by 06:00 UTC"). Alert on SLA breach, not just on failure.

  - id: data-validation
    tier: recommended
    title: "Data Validation & Quality"
    content: |
      ## Data Validation & Quality Gates

      - Validate data at every boundary: after extraction (schema conformance), after transformation (business rules), and before loading (referential integrity).
      - Use schema contracts (e.g., JSON Schema, Avro, Protobuf, or Great Expectations suites) to enforce column types, nullability, and allowed value ranges.
      - Implement row-count and distribution checks between stages. A sudden drop or spike in row count (> 20% deviation from baseline) should trigger an alert and pause downstream processing.
      - Quarantine invalid records into a dead-letter table rather than dropping them silently. Include the original record, the validation error, and the run ID.
      - Maintain a data quality dashboard that tracks freshness, completeness, uniqueness, and accuracy metrics per dataset over time.
      - Write unit tests for transformation logic using fixed input fixtures. Write integration tests that run a mini-pipeline against a test database.

  - id: reliability-patterns
    tier: recommended
    title: "Reliability & Error Handling"
    content: |
      ## Reliability, Retry & Recovery

      - Configure retries with exponential backoff and jitter for transient failures (network timeouts, rate limits, temporary unavailability). Cap retries at 3-5 attempts.
      - Use checkpointing or write-ahead logs for long-running pipelines so that a restart resumes from the last successful checkpoint, not from the beginning.
      - Implement circuit breakers on external API calls to prevent cascading failures when a source system is degraded.
      - Log structured events (JSON) for every stage: start, success, failure, retry, and skip. Include record counts, duration, and error details.
      - Design for exactly-once semantics where possible using upserts (INSERT ... ON CONFLICT UPDATE) or deduplication keys in the target store.
      - Maintain a pipeline runbook that documents failure modes, recovery steps, and escalation contacts. Review and update it quarterly.
